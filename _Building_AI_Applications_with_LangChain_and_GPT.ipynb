{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFSuXYJpW+DREK5yfvGLps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphaelroosewelt/langChain/blob/main/_Building_AI_Applications_with_LangChain_and_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To perform this analysis, we need to install the following packages:\n",
        "#### **openai**: For interacting with OpenAI's API.\n",
        "#### **langchain**: A framework for developing applications with generative AI.\n",
        "#### **langchain-openai** and **langchain-community**: LangChain extension modules for OpenAI and DuckDB functionality.\n",
        "#### **langgraph**: A package to orchestrate LLM systems.\n",
        "#### **tiktoken**: A string encoder that generates tokens used by OpenAI, useful for estimating token usage.\n",
        "#### **duckdb**: We will use DuckDB as a vector database."
      ],
      "metadata": {
        "id": "pe4MdhF0MIvD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "pebPH5p3KLiH"
      },
      "outputs": [],
      "source": [
        "# We tested the code-along with the following package versions\n",
        "!pip install openai==1.63.2 \\\n",
        "             langchain==0.3.19 \\\n",
        "             langchain-core==0.3.40 \\\n",
        "\t\t\t       langchain-openai==0.3.6 \\\n",
        "\t\t\t       langchain-community==0.3.18 \\\n",
        "             langgraph==0.2.74 \\\n",
        "\t\t\t       tiktoken==0.9.0 \\\n",
        "             unstructured[all-docs] \\\n",
        "             typing_extensions==4.12.2 \\\n",
        " \t\t\t       duckdb==1.2.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinstalar numpy e pandas com versões compatíveis\n",
        "!pip install numpy==1.24.4 pandas==2.1.4 --force-reinstall --no-cache-dir > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "F6uJtqUxvxNv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Load Data\n",
        "\n",
        "To embed and store data, we need to provide LangChain with `Document` objects. This can be easily achieved using LangChain's [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/). For our project, we will use the `ReadTheDocsLoader` to load the scikit-learn documentation. The documentation files are located in the `sckit-learn-docs` folder, which contains all the HTML files from the scikit-learn documentation (https://scikit-learn.org/dev/versions.html).\n",
        "\n",
        "Our goal is to load these HTML files as `Document` objects using the `ReadTheDocsLoader`. This loader will read the directory containing the HTML files, strip out the HTML tags, and convert the content into `Document` objects. By the end of this task, we will have a variable `raw_documents` that contains a list of `Document` objects, with each `Document` corresponding to an HTML file.\n",
        "\n",
        "Note: In this step, we are not loading the documents into a database; we are simply loading them into a list.\n",
        "\n",
        "### Instructions\n",
        "1. Import `ReadTheDocsLoader` from `langchain.document_loaders`.\n",
        "2. Create the loader, pointing to the `sckit-learn-docs` directory.\n",
        "3. Load the data into `raw_documents` by calling `loader.load()`."
      ],
      "metadata": {
        "id": "8M4HSFegsACi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjdii9YZK0Za",
        "outputId": "a79bcc49-4a1c-4e91-e2c8-3b55aba75697"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"TokenAgentKey\"] = userdata.get(\"TokenAgentKey\")"
      ],
      "metadata": {
        "id": "Y2vsWtR3briy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Caminho base\n",
        "path = '/content/drive/My Drive/Docs/scikit-learn-docs'\n",
        "\n",
        "# Listar conteúdo\n",
        "for root, dirs, files in os.walk(path):\n",
        "    print(f\"📁 {root}\")\n",
        "    for file in files:\n",
        "        print(f\"   └── {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dbzqTHbuqp91",
        "outputId": "075f6a62-462e-4303-9f75-106d939d741f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 /content/drive/My Drive/Docs/scikit-learn-docs\n",
            "📁 /content/drive/My Drive/Docs/scikit-learn-docs/api\n",
            "   └── sklearn.kernel_approximation.html\n",
            "   └── sklearn.covariance.html\n",
            "   └── sklearn.ensemble.html\n",
            "   └── sklearn.base.html\n",
            "   └── deprecated.html\n",
            "   └── sklearn.semi_supervised.html\n",
            "   └── sklearn.neighbors.html\n",
            "   └── sklearn.metrics.html\n",
            "   └── sklearn.linear_model.html\n",
            "   └── sklearn.kernel_ridge.html\n",
            "   └── sklearn.utils.html\n",
            "   └── sklearn.gaussian_process.html\n",
            "   └── sklearn.tree.html\n",
            "   └── sklearn.isotonic.html\n",
            "   └── sklearn.calibration.html\n",
            "   └── sklearn.multiclass.html\n",
            "   └── sklearn.frozen.html\n",
            "   └── sklearn.neural_network.html\n",
            "   └── sklearn.feature_selection.html\n",
            "   └── sklearn.datasets.html\n",
            "   └── sklearn.random_projection.html\n",
            "   └── sklearn.decomposition.html\n",
            "   └── sklearn.discriminant_analysis.html\n",
            "   └── sklearn.svm.html\n",
            "   └── index.html\n",
            "   └── sklearn.inspection.html\n",
            "   └── sklearn.manifold.html\n",
            "   └── sklearn.cluster.html\n",
            "   └── sklearn.html\n",
            "   └── sklearn.exceptions.html\n",
            "   └── sklearn.feature_extraction.html\n",
            "   └── sklearn.preprocessing.html\n",
            "   └── sklearn.impute.html\n",
            "   └── sklearn.multioutput.html\n",
            "   └── sklearn.dummy.html\n",
            "   └── sklearn.cross_decomposition.html\n",
            "   └── sklearn.experimental.html\n",
            "   └── sklearn.model_selection.html\n",
            "   └── sklearn.mixture.html\n",
            "   └── sklearn.pipeline.html\n",
            "   └── sklearn.compose.html\n",
            "   └── sklearn.naive_bayes.html\n",
            "📁 /content/drive/My Drive/Docs/scikit-learn-docs/modules\n",
            "   └── neighbors.html\n",
            "   └── decomposition.html\n",
            "   └── gaussian_process.html\n",
            "   └── feature_selection.html\n",
            "   └── permutation_importance.html\n",
            "   └── random_projection.html\n",
            "   └── model_persistence.html\n",
            "   └── preprocessing_targets.html\n",
            "   └── multiclass.html\n",
            "   └── sgd.html\n",
            "   └── semi_supervised.html\n",
            "   └── grid_search.html\n",
            "   └── unsupervised_reduction.html\n",
            "   └── covariance.html\n",
            "   └── neural_networks_unsupervised.html\n",
            "   └── kernel_approximation.html\n",
            "   └── biclustering.html\n",
            "   └── mixture.html\n",
            "   └── linear_model.html\n",
            "   └── lda_qda.html\n",
            "   └── pipeline.html\n",
            "   └── kernel_ridge.html\n",
            "   └── svm.html\n",
            "   └── classes.html\n",
            "   └── compose.html\n",
            "   └── ensemble.html\n",
            "   └── metrics.html\n",
            "   └── partial_dependence.html\n",
            "   └── density.html\n",
            "   └── classification_threshold.html\n",
            "   └── cross_validation.html\n",
            "   └── calibration.html\n",
            "   └── manifold.html\n",
            "   └── naive_bayes.html\n",
            "   └── cross_decomposition.html\n",
            "   └── learning_curve.html\n",
            "   └── outlier_detection.html\n",
            "   └── array_api.html\n",
            "   └── model_evaluation.html\n",
            "   └── clustering.html\n",
            "   └── preprocessing.html\n",
            "   └── isotonic.html\n",
            "   └── tree.html\n",
            "   └── neural_networks_supervised.html\n",
            "   └── impute.html\n",
            "   └── feature_extraction.html\n",
            "📁 /content/drive/My Drive/Docs/scikit-learn-docs/modules/generated\n",
            "   └── sklearn.isotonic.IsotonicRegression.html\n",
            "   └── sklearn.impute.KNNImputer.html\n",
            "   └── sklearn.datasets.fetch_20newsgroups.html\n",
            "   └── sklearn.naive_bayes.GaussianNB.html\n",
            "   └── sklearn.datasets.make_spd_matrix.html\n",
            "   └── sklearn.metrics.class_likelihood_ratios.html\n",
            "   └── sklearn.cluster.compute_optics_graph.html\n",
            "   └── sklearn.cluster.MeanShift.html\n",
            "   └── sklearn.naive_bayes.MultinomialNB.html\n",
            "   └── sklearn.covariance.ShrunkCovariance.html\n",
            "   └── sklearn.datasets.clear_data_home.html\n",
            "   └── sklearn.utils.class_weight.compute_sample_weight.html\n",
            "   └── sklearn.decomposition.PCA.html\n",
            "   └── sklearn.model_selection.LearningCurveDisplay.html\n",
            "   └── sklearn.calibration.CalibratedClassifierCV.html\n",
            "   └── sklearn.cluster.KMeans.html\n",
            "   └── sklearn.compose.make_column_selector.html\n",
            "   └── sklearn.preprocessing.OneHotEncoder.html\n",
            "   └── sklearn.svm.SVR.html\n",
            "   └── sklearn.utils.Bunch.html\n",
            "   └── sklearn.utils.metadata_routing.MetadataRequest.html\n",
            "   └── sklearn.gaussian_process.kernels.CompoundKernel.html\n",
            "   └── sklearn.random_projection.SparseRandomProjection.html\n",
            "   └── sklearn.utils.sparsefuncs.mean_variance_axis.html\n",
            "   └── sklearn.metrics.d2_pinball_score.html\n",
            "   └── sklearn.utils.check_consistent_length.html\n",
            "   └── sklearn.covariance.GraphicalLassoCV.html\n",
            "   └── sklearn.metrics.silhouette_samples.html\n",
            "   └── sklearn.datasets.load_linnerud.html\n",
            "   └── sklearn.model_selection.TunedThresholdClassifierCV.html\n",
            "   └── sklearn.cluster.BisectingKMeans.html\n",
            "   └── sklearn.feature_extraction.image.PatchExtractor.html\n",
            "   └── sklearn.kernel_approximation.RBFSampler.html\n",
            "   └── sklearn.feature_selection.SequentialFeatureSelector.html\n",
            "   └── sklearn.utils.parallel.Parallel.html\n",
            "   └── sklearn.feature_extraction.text.TfidfVectorizer.html\n",
            "   └── sklearn.utils.multiclass.unique_labels.html\n",
            "   └── sklearn.utils.sparsefuncs.inplace_swap_row.html\n",
            "   └── sklearn.cluster.AgglomerativeClustering.html\n",
            "   └── sklearn.metrics.check_scoring.html\n",
            "   └── sklearn.metrics.root_mean_squared_log_error.html\n",
            "   └── sklearn.model_selection.cross_validate.html\n",
            "   └── sklearn.metrics.f1_score.html\n",
            "   └── sklearn.datasets.load_diabetes.html\n",
            "   └── sklearn.metrics.precision_recall_curve.html\n",
            "   └── sklearn.metrics.v_measure_score.html\n",
            "   └── sklearn.metrics.log_loss.html\n",
            "   └── sklearn.linear_model.lasso_path.html\n",
            "   └── sklearn.linear_model.lars_path.html\n",
            "   └── sklearn.metrics.multilabel_confusion_matrix.html\n",
            "   └── sklearn.tree.ExtraTreeClassifier.html\n",
            "   └── sklearn.feature_selection.SelectFpr.html\n",
            "   └── sklearn.utils.register_parallel_backend.html\n",
            "   └── sklearn.neighbors.KNeighborsRegressor.html\n",
            "   └── sklearn.metrics.d2_log_loss_score.html\n",
            "   └── sklearn.utils.assert_all_finite.html\n",
            "   └── sklearn.metrics.pairwise_distances_chunked.html\n",
            "   └── sklearn.datasets.fetch_lfw_pairs.html\n",
            "   └── sklearn.metrics.mean_pinball_loss.html\n",
            "   └── sklearn.metrics.normalized_mutual_info_score.html\n",
            "   └── sklearn.svm.NuSVC.html\n",
            "   └── sklearn.utils.metadata_routing.MetadataRouter.html\n",
            "   └── sklearn.model_selection.validation_curve.html\n",
            "   └── sklearn.utils.parallel_backend.html\n",
            "   └── sklearn.preprocessing.MultiLabelBinarizer.html\n",
            "   └── sklearn.cluster.HDBSCAN.html\n",
            "   └── sklearn.pipeline.make_pipeline.html\n",
            "   └── sklearn.datasets.load_svmlight_file.html\n",
            "   └── sklearn.metrics.top_k_accuracy_score.html\n",
            "   └── sklearn.metrics.pairwise_distances_argmin.html\n",
            "   └── sklearn.base.ClassifierMixin.html\n",
            "   └── sklearn.datasets.make_hastie_10_2.html\n",
            "   └── sklearn.neural_network.MLPRegressor.html\n",
            "   └── sklearn.metrics.pairwise.sigmoid_kernel.html\n",
            "   └── sklearn.linear_model.MultiTaskElasticNet.html\n",
            "   └── sklearn.gaussian_process.kernels.RBF.html\n",
            "   └── sklearn.model_selection.StratifiedKFold.html\n",
            "   └── sklearn.preprocessing.normalize.html\n",
            "   └── sklearn.linear_model.SGDRegressor.html\n",
            "   └── sklearn.metrics.mutual_info_score.html\n",
            "   └── sklearn.utils.ClassifierTags.html\n",
            "   └── sklearn.dummy.DummyClassifier.html\n",
            "   └── sklearn.preprocessing.minmax_scale.html\n",
            "   └── sklearn.metrics.recall_score.html\n",
            "   └── sklearn.metrics.mean_squared_error.html\n",
            "   └── sklearn.utils.estimator_checks.estimator_checks_generator.html\n",
            "   └── sklearn.utils.class_weight.compute_class_weight.html\n",
            "   └── sklearn.inspection.permutation_importance.html\n",
            "   └── sklearn.utils.estimator_html_repr.html\n",
            "   └── sklearn.utils.safe_sqr.html\n",
            "   └── sklearn.cluster.cluster_optics_xi.html\n",
            "   └── sklearn.feature_extraction.text.CountVectorizer.html\n",
            "   └── sklearn.svm.LinearSVC.html\n",
            "   └── sklearn.decomposition.MiniBatchNMF.html\n",
            "   └── sklearn.utils.indexable.html\n",
            "   └── sklearn.feature_selection.SelectFdr.html\n",
            "   └── sklearn.exceptions.EfficiencyWarning.html\n",
            "   └── sklearn.metrics.accuracy_score.html\n",
            "   └── sklearn.linear_model.HuberRegressor.html\n",
            "   └── sklearn.linear_model.Perceptron.html\n",
            "   └── sklearn.datasets.make_s_curve.html\n",
            "   └── sklearn.model_selection.ParameterGrid.html\n",
            "   └── sklearn.cross_decomposition.CCA.html\n",
            "   └── sklearn.neighbors.RadiusNeighborsTransformer.html\n",
            "   └── sklearn.datasets.fetch_openml.html\n",
            "   └── sklearn.neural_network.BernoulliRBM.html\n",
            "   └── sklearn.gaussian_process.kernels.WhiteKernel.html\n",
            "   └── sklearn.ensemble.AdaBoostClassifier.html\n",
            "   └── sklearn.model_selection.LeaveOneOut.html\n",
            "   └── sklearn.preprocessing.KernelCenterer.html\n",
            "   └── sklearn.utils.check_array.html\n",
            "   └── sklearn.datasets.make_friedman1.html\n",
            "   └── sklearn.cluster.DBSCAN.html\n",
            "   └── sklearn.cross_decomposition.PLSCanonical.html\n",
            "   └── sklearn.multioutput.MultiOutputClassifier.html\n",
            "   └── sklearn.preprocessing.RobustScaler.html\n",
            "   └── sklearn.exceptions.NotFittedError.html\n",
            "   └── sklearn.utils.metadata_routing.process_routing.html\n",
            "   └── sklearn.model_selection.cross_val_score.html\n",
            "   └── sklearn.base.BiclusterMixin.html\n",
            "   └── sklearn.linear_model.Lasso.html\n",
            "   └── sklearn.linear_model.Lars.html\n",
            "   └── sklearn.metrics.pairwise.polynomial_kernel.html\n",
            "   └── sklearn.preprocessing.FunctionTransformer.html\n",
            "   └── sklearn.utils.validation.validate_data.html\n",
            "   └── sklearn.feature_extraction.image.img_to_graph.html\n",
            "   └── sklearn.feature_extraction.DictVectorizer.html\n",
            "   └── sklearn.preprocessing.OrdinalEncoder.html\n",
            "   └── sklearn.utils.TransformerTags.html\n",
            "   └── sklearn.linear_model.LogisticRegressionCV.html\n",
            "   └── sklearn.linear_model.TheilSenRegressor.html\n",
            "   └── sklearn.utils.sparsefuncs.inplace_csr_column_scale.html\n",
            "   └── sklearn.covariance.ledoit_wolf.html\n",
            "   └── sklearn.metrics.pairwise.rbf_kernel.html\n",
            "   └── sklearn.preprocessing.KBinsDiscretizer.html\n",
            "   └── sklearn.model_selection.GroupKFold.html\n",
            "   └── sklearn.datasets.fetch_olivetti_faces.html\n",
            "   └── sklearn.compose.make_column_transformer.html\n",
            "   └── sklearn.exceptions.EstimatorCheckFailedWarning.html\n",
            "   └── sklearn.datasets.make_checkerboard.html\n",
            "   └── sklearn.linear_model.ARDRegression.html\n",
            "   └── sklearn.manifold.smacof.html\n",
            "   └── sklearn.manifold.Isomap.html\n",
            "   └── sklearn.mixture.BayesianGaussianMixture.html\n",
            "   └── sklearn.base.TransformerMixin.html\n",
            "   └── sklearn.base.is_outlier_detector.html\n",
            "   └── sklearn.feature_selection.SelectFwe.html\n",
            "   └── sklearn.base.OneToOneFeatureMixin.html\n",
            "   └── sklearn.feature_extraction.FeatureHasher.html\n",
            "   └── sklearn.ensemble.GradientBoostingClassifier.html\n",
            "   └── sklearn.feature_selection.r_regression.html\n",
            "   └── sklearn.tree.DecisionTreeClassifier.html\n",
            "   └── sklearn.gaussian_process.kernels.DotProduct.html\n",
            "   └── sklearn.decomposition.DictionaryLearning.html\n",
            "   └── sklearn.decomposition.LatentDirichletAllocation.html\n",
            "   └── sklearn.multiclass.OneVsOneClassifier.html\n",
            "   └── sklearn.inspection.partial_dependence.html\n",
            "   └── sklearn.decomposition.IncrementalPCA.html\n",
            "   └── sklearn.set_config.html\n",
            "   └── sklearn.metrics.consensus_score.html\n",
            "   └── sklearn.gaussian_process.kernels.RationalQuadratic.html\n",
            "   └── sklearn.utils.extmath.randomized_svd.html\n",
            "   └── sklearn.ensemble.RandomForestRegressor.html\n",
            "   └── sklearn.datasets.make_sparse_uncorrelated.html\n",
            "   └── sklearn.linear_model.enet_path.html\n",
            "   └── sklearn.metrics.pairwise.paired_distances.html\n",
            "   └── sklearn.utils.safe_mask.html\n",
            "   └── sklearn.preprocessing.add_dummy_feature.html\n",
            "   └── sklearn.neighbors.NeighborhoodComponentsAnalysis.html\n",
            "   └── sklearn.linear_model.MultiTaskLasso.html\n",
            "   └── sklearn.datasets.make_circles.html\n",
            "   └── sklearn.base.is_classifier.html\n",
            "   └── sklearn.metrics.fowlkes_mallows_score.html\n",
            "   └── sklearn.datasets.load_wine.html\n",
            "   └── sklearn.metrics.calinski_harabasz_score.html\n",
            "   └── sklearn.model_selection.ParameterSampler.html\n",
            "   └── sklearn.linear_model.RidgeClassifier.html\n",
            "   └── sklearn.preprocessing.TargetEncoder.html\n",
            "   └── sklearn.covariance.empirical_covariance.html\n",
            "   └── sklearn.show_versions.html\n",
            "   └── sklearn.metrics.pairwise.paired_cosine_distances.html\n",
            "   └── sklearn.linear_model.LassoLarsIC.html\n",
            "   └── sklearn.linear_model.PassiveAggressiveClassifier.html\n",
            "   └── sklearn.feature_selection.f_classif.html\n",
            "   └── sklearn.semi_supervised.LabelPropagation.html\n",
            "   └── sklearn.cluster.SpectralBiclustering.html\n",
            "   └── sklearn.impute.MissingIndicator.html\n",
            "   └── sklearn.model_selection.LeavePOut.html\n",
            "   └── sklearn.datasets.dump_svmlight_file.html\n",
            "   └── sklearn.ensemble.HistGradientBoostingClassifier.html\n",
            "   └── sklearn.exceptions.DataDimensionalityWarning.html\n",
            "   └── sklearn.pipeline.FeatureUnion.html\n",
            "   └── sklearn.datasets.load_files.html\n",
            "   └── sklearn.multioutput.RegressorChain.html\n",
            "   └── sklearn.metrics.cohen_kappa_score.html\n",
            "   └── sklearn.metrics.pairwise.haversine_distances.html\n",
            "   └── sklearn.utils.graph.single_source_shortest_path_length.html\n",
            "   └── sklearn.model_selection.GridSearchCV.html\n",
            "   └── sklearn.ensemble.StackingRegressor.html\n",
            "   └── sklearn.feature_selection.f_regression.html\n",
            "   └── sklearn.impute.IterativeImputer.html\n",
            "   └── sklearn.utils.TargetTags.html\n",
            "   └── sklearn.cluster.FeatureAgglomeration.html\n",
            "   └── sklearn.get_config.html\n",
            "   └── sklearn.datasets.make_low_rank_matrix.html\n",
            "   └── sklearn.feature_selection.VarianceThreshold.html\n",
            "   └── sklearn.neighbors.NearestCentroid.html\n",
            "   └── sklearn.kernel_ridge.KernelRidge.html\n",
            "   └── sklearn.model_selection.LeavePGroupsOut.html\n",
            "   └── sklearn.preprocessing.maxabs_scale.html\n",
            "   └── sklearn.datasets.make_regression.html\n",
            "   └── sklearn.exceptions.ConvergenceWarning.html\n",
            "   └── sklearn.manifold.SpectralEmbedding.html\n",
            "   └── sklearn.metrics.get_scorer_names.html\n",
            "   └── sklearn.compose.ColumnTransformer.html\n",
            "   └── sklearn.preprocessing.Normalizer.html\n",
            "   └── sklearn.mixture.GaussianMixture.html\n",
            "   └── sklearn.ensemble.VotingClassifier.html\n",
            "   └── sklearn.linear_model.SGDOneClassSVM.html\n",
            "   └── sklearn.metrics.pairwise.cosine_distances.html\n",
            "   └── sklearn.feature_selection.chi2.html\n",
            "   └── sklearn.feature_selection.RFECV.html\n",
            "   └── sklearn.preprocessing.label_binarize.html\n",
            "   └── sklearn.metrics.rand_score.html\n",
            "   └── sklearn.ensemble.ExtraTreesClassifier.html\n",
            "   └── sklearn.covariance.graphical_lasso.html\n",
            "   └── sklearn.cluster.affinity_propagation.html\n",
            "   └── sklearn.linear_model.LinearRegression.html\n",
            "   └── sklearn.preprocessing.PolynomialFeatures.html\n",
            "   └── sklearn.neighbors.RadiusNeighborsClassifier.html\n",
            "   └── sklearn.linear_model.ElasticNet.html\n",
            "   └── sklearn.feature_selection.SelectorMixin.html\n",
            "   └── sklearn.datasets.make_swiss_roll.html\n",
            "   └── sklearn.linear_model.RANSACRegressor.html\n",
            "   └── sklearn.metrics.confusion_matrix.html\n",
            "   └── sklearn.metrics.DetCurveDisplay.html\n",
            "   └── sklearn.datasets.make_friedman3.html\n",
            "   └── sklearn.exceptions.InconsistentVersionWarning.html\n",
            "   └── sklearn.decomposition.SparsePCA.html\n",
            "   └── sklearn.datasets.fetch_kddcup99.html\n",
            "   └── sklearn.linear_model.lars_path_gram.html\n",
            "   └── sklearn.covariance.EllipticEnvelope.html\n",
            "   └── sklearn.datasets.load_sample_images.html\n",
            "   └── sklearn.kernel_approximation.AdditiveChi2Sampler.html\n",
            "   └── sklearn.linear_model.LassoLars.html\n",
            "   └── sklearn.utils.check_random_state.html\n",
            "   └── sklearn.datasets.make_sparse_coded_signal.html\n",
            "   └── sklearn.base.is_clusterer.html\n",
            "   └── sklearn.utils.sparsefuncs.inplace_swap_column.html\n",
            "   └── sklearn.linear_model.RidgeCV.html\n",
            "   └── sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l1.html\n",
            "   └── sklearn.base.MetaEstimatorMixin.html\n",
            "   └── sklearn.cluster.ward_tree.html\n",
            "   └── sklearn.utils.get_tags.html\n",
            "   └── sklearn.datasets.fetch_20newsgroups_vectorized.html\n",
            "   └── sklearn.metrics.pairwise.chi2_kernel.html\n",
            "   └── sklearn.neighbors.LocalOutlierFactor.html\n",
            "   └── sklearn.metrics.roc_curve.html\n",
            "   └── sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\n",
            "   └── sklearn.datasets.fetch_lfw_people.html\n",
            "   └── sklearn.utils.extmath.weighted_mode.html\n",
            "   └── sklearn.cluster.cluster_optics_dbscan.html\n",
            "   └── sklearn.multiclass.OneVsRestClassifier.html\n",
            "   └── sklearn.metrics.d2_absolute_error_score.html\n",
            "   └── sklearn.ensemble.BaggingClassifier.html\n",
            "   └── sklearn.model_selection.ShuffleSplit.html\n",
            "   └── sklearn.feature_extraction.text.TfidfTransformer.html\n",
            "   └── sklearn.metrics.mean_absolute_error.html\n",
            "   └── sklearn.linear_model.OrthogonalMatchingPursuit.html\n",
            "   └── sklearn.gaussian_process.GaussianProcessRegressor.html\n",
            "   └── sklearn.utils.deprecated.html\n",
            "   └── sklearn.preprocessing.StandardScaler.html\n",
            "   └── sklearn.multiclass.OutputCodeClassifier.html\n",
            "   └── sklearn.cluster.spectral_clustering.html\n",
            "   └── sklearn.datasets.load_iris.html\n",
            "   └── sklearn.model_selection.RepeatedStratifiedKFold.html\n",
            "   └── sklearn.model_selection.HalvingRandomSearchCV.html\n",
            "   └── sklearn.datasets.make_moons.html\n",
            "   └── sklearn.manifold.trustworthiness.html\n",
            "   └── sklearn.decomposition.MiniBatchDictionaryLearning.html\n",
            "   └── sklearn.preprocessing.SplineTransformer.html\n",
            "   └── sklearn.gaussian_process.kernels.Matern.html\n",
            "   └── fastica-function.html\n",
            "   └── sklearn.datasets.fetch_file.html\n",
            "   └── sklearn.cluster.estimate_bandwidth.html\n",
            "   └── sklearn.model_selection.StratifiedGroupKFold.html\n",
            "   └── sklearn.semi_supervised.SelfTrainingClassifier.html\n",
            "   └── sklearn.base.ClusterMixin.html\n",
            "   └── sklearn.svm.SVC.html\n",
            "   └── sklearn.model_selection.RandomizedSearchCV.html\n",
            "   └── sklearn.utils.validation.check_is_fitted.html\n",
            "   └── sklearn.datasets.make_classification.html\n",
            "   └── sklearn.manifold.locally_linear_embedding.html\n",
            "   └── sklearn.utils.arrayfuncs.min_pos.html\n",
            "   └── sklearn.base.is_regressor.html\n",
            "   └── sklearn.naive_bayes.BernoulliNB.html\n",
            "   └── sklearn.linear_model.PassiveAggressiveRegressor.html\n",
            "   └── sklearn.metrics.precision_recall_fscore_support.html\n",
            "   └── sklearn.ensemble.HistGradientBoostingRegressor.html\n",
            "   └── sklearn.preprocessing.quantile_transform.html\n",
            "   └── sklearn.neighbors.KernelDensity.html\n",
            "   └── sklearn.decomposition.KernelPCA.html\n",
            "   └── sklearn.cluster.SpectralCoclustering.html\n",
            "   └── sklearn.metrics.hinge_loss.html\n",
            "   └── dbscan-function.html\n",
            "   └── sklearn.metrics.pairwise.manhattan_distances.html\n",
            "   └── sklearn.cross_decomposition.PLSSVD.html\n",
            "   └── sklearn.neighbors.KNeighborsClassifier.html\n",
            "   └── sklearn.linear_model.PoissonRegressor.html\n",
            "   └── sklearn.metrics.make_scorer.html\n",
            "   └── sklearn.preprocessing.MaxAbsScaler.html\n",
            "   └── sklearn.linear_model.BayesianRidge.html\n",
            "   └── sklearn.linear_model.orthogonal_mp_gram.html\n",
            "   └── sklearn.utils.extmath.density.html\n",
            "   └── sklearn.preprocessing.QuantileTransformer.html\n",
            "   └── sklearn.inspection.PartialDependenceDisplay.html\n",
            "   └── sklearn.preprocessing.robust_scale.html\n",
            "   └── sklearn.kernel_approximation.PolynomialCountSketch.html\n",
            "   └── sklearn.cluster.Birch.html\n",
            "   └── sklearn.neighbors.sort_graph_by_row_values.html\n",
            "   └── sklearn.ensemble.GradientBoostingRegressor.html\n",
            "   └── sklearn.linear_model.QuantileRegressor.html\n",
            "   └── sklearn.metrics.davies_bouldin_score.html\n",
            "   └── sklearn.tree.DecisionTreeRegressor.html\n",
            "   └── sklearn.feature_selection.SelectKBest.html\n",
            "   └── sklearn.datasets.get_data_home.html\n",
            "   └── sklearn.preprocessing.Binarizer.html\n",
            "   └── sklearn.preprocessing.LabelBinarizer.html\n",
            "   └── sklearn.experimental.enable_halving_search_cv.html\n",
            "   └── sklearn.linear_model.LogisticRegression.html\n",
            "   └── sklearn.model_selection.LeaveOneGroupOut.html\n",
            "   └── sklearn.model_selection.learning_curve.html\n",
            "   └── sklearn.utils.discovery.all_estimators.html\n",
            "   └── sklearn.utils.gen_even_slices.html\n",
            "   └── sklearn.utils.metadata_routing.get_routing_for_object.html\n",
            "   └── sklearn.utils.extmath.randomized_range_finder.html\n",
            "   └── sklearn.metrics.adjusted_rand_score.html\n",
            "   └── sklearn.neighbors.radius_neighbors_graph.html\n",
            "   └── sklearn.metrics.mean_squared_log_error.html\n",
            "   └── oas-function.html\n",
            "   └── sklearn.naive_bayes.ComplementNB.html\n",
            "   └── sklearn.metrics.homogeneity_score.html\n",
            "   └── sklearn.metrics.jaccard_score.html\n",
            "   └── sklearn.metrics.pairwise.distance_metrics.html\n",
            "   └── sklearn.metrics.RocCurveDisplay.html\n",
            "   └── sklearn.feature_selection.GenericUnivariateSelect.html\n",
            "   └── sklearn.linear_model.GammaRegressor.html\n",
            "   └── sklearn.utils.resample.html\n",
            "   └── sklearn.metrics.cluster.pair_confusion_matrix.html\n",
            "   └── sklearn.utils.extmath.fast_logdet.html\n",
            "   └── sklearn.metrics.balanced_accuracy_score.html\n",
            "   └── sklearn.metrics.DistanceMetric.html\n",
            "   └── sklearn.datasets.fetch_california_housing.html\n",
            "   └── sklearn.metrics.r2_score.html\n",
            "   └── sklearn.utils.sparsefuncs.inplace_row_scale.html\n",
            "   └── sklearn.metrics.brier_score_loss.html\n",
            "   └── sklearn.feature_extraction.image.grid_to_graph.html\n",
            "   └── sklearn.model_selection.KFold.html\n",
            "   └── sklearn.decomposition.non_negative_factorization.html\n",
            "   └── sklearn.metrics.ConfusionMatrixDisplay.html\n",
            "   └── sklearn.neighbors.RadiusNeighborsRegressor.html\n",
            "   └── sklearn.datasets.make_biclusters.html\n",
            "   └── sklearn.feature_selection.SelectFromModel.html\n",
            "   └── sklearn.linear_model.Ridge.html\n",
            "   └── sklearn.linear_model.LarsCV.html\n",
            "   └── sklearn.metrics.pairwise.paired_manhattan_distances.html\n",
            "   └── sklearn.tree.export_text.html\n",
            "   └── sklearn.experimental.enable_iterative_imputer.html\n",
            "   └── sklearn.gaussian_process.kernels.ConstantKernel.html\n",
            "   └── sklearn.calibration.CalibrationDisplay.html\n",
            "   └── sklearn.metrics.max_error.html\n",
            "   └── sklearn.random_projection.johnson_lindenstrauss_min_dim.html\n",
            "   └── sklearn.linear_model.LassoCV.html\n",
            "   └── sklearn.manifold.spectral_embedding.html\n",
            "   └── sklearn.utils.multiclass.is_multilabel.html\n",
            "   └── sklearn.cross_decomposition.PLSRegression.html\n",
            "   └── sklearn.random_projection.GaussianRandomProjection.html\n",
            "   └── sklearn.ensemble.BaggingRegressor.html\n",
            "   └── sklearn.metrics.homogeneity_completeness_v_measure.html\n",
            "   └── sklearn.utils.estimator_checks.parametrize_with_checks.html\n",
            "   └── sklearn.metrics.matthews_corrcoef.html\n",
            "   └── sklearn.linear_model.ridge_regression.html\n",
            "   └── sklearn.datasets.make_gaussian_quantiles.html\n",
            "   └── sklearn.ensemble.ExtraTreesRegressor.html\n",
            "   └── sklearn.decomposition.MiniBatchSparsePCA.html\n",
            "   └── sklearn.metrics.classification_report.html\n",
            "   └── sklearn.neural_network.MLPClassifier.html\n",
            "   └── sklearn.covariance.ledoit_wolf_shrinkage.html\n",
            "   └── sklearn.metrics.precision_score.html\n",
            "   └── sklearn.neighbors.kneighbors_graph.html\n",
            "   └── sklearn.datasets.make_sparse_spd_matrix.html\n",
            "   └── sklearn.metrics.ndcg_score.html\n",
            "   └── sklearn.linear_model.TweedieRegressor.html\n",
            "   └── sklearn.compose.TransformedTargetRegressor.html\n",
            "   └── sklearn.feature_extraction.text.HashingVectorizer.html\n",
            "   └── sklearn.covariance.OAS.html\n",
            "   └── sklearn.model_selection.train_test_split.html\n",
            "   └── sklearn.decomposition.NMF.html\n",
            "   └── sklearn.exceptions.UndefinedMetricWarning.html\n",
            "   └── sklearn.utils.shuffle.html\n",
            "   └── sklearn.datasets.fetch_rcv1.html\n",
            "   └── sklearn.metrics.roc_auc_score.html\n",
            "   └── sklearn.ensemble.VotingRegressor.html\n",
            "   └── sklearn.metrics.pairwise.laplacian_kernel.html\n",
            "   └── sklearn.utils.check_X_y.html\n",
            "   └── sklearn.linear_model.MultiTaskElasticNetCV.html\n",
            "   └── sklearn.manifold.MDS.html\n",
            "   └── sklearn.tree.plot_tree.html\n",
            "   └── sklearn.base.BaseEstimator.html\n",
            "   └── sklearn.utils.sparsefuncs.incr_mean_variance_axis.html\n",
            "   └── sklearn.linear_model.SGDClassifier.html\n",
            "   └── sklearn.metrics.d2_tweedie_score.html\n",
            "   └── sklearn.datasets.make_friedman2.html\n",
            "   └── sklearn.feature_selection.RFE.html\n",
            "   └── sklearn.ensemble.RandomTreesEmbedding.html\n",
            "   └── sklearn.linear_model.MultiTaskLassoCV.html\n",
            "   └── sklearn.covariance.MinCovDet.html\n",
            "   └── sklearn.model_selection.StratifiedShuffleSplit.html\n",
            "   └── sklearn.svm.NuSVR.html\n",
            "   └── sklearn.manifold.LocallyLinearEmbedding.html\n",
            "   └── sklearn.multioutput.MultiOutputRegressor.html\n",
            "   └── sklearn.model_selection.HalvingGridSearchCV.html\n",
            "   └── sklearn.linear_model.orthogonal_mp.html\n",
            "   └── sklearn.semi_supervised.LabelSpreading.html\n",
            "   └── sklearn.ensemble.RandomForestClassifier.html\n",
            "   └── sklearn.metrics.completeness_score.html\n",
            "   └── sklearn.naive_bayes.CategoricalNB.html\n",
            "   └── sklearn.decomposition.TruncatedSVD.html\n",
            "   └── sklearn.metrics.explained_variance_score.html\n",
            "   └── sklearn.dummy.DummyRegressor.html\n",
            "   └── sklearn.utils.parallel.delayed.html\n",
            "   └── sklearn.ensemble.AdaBoostRegressor.html\n",
            "   └── sklearn.covariance.LedoitWolf.html\n",
            "   └── sklearn.utils._safe_indexing.html\n",
            "   └── sklearn.metrics.pairwise.linear_kernel.html\n",
            "   └── sklearn.svm.OneClassSVM.html\n",
            "   └── sklearn.base.OutlierMixin.html\n",
            "   └── sklearn.neighbors.NearestNeighbors.html\n",
            "   └── sklearn.linear_model.RidgeClassifierCV.html\n",
            "   └── sklearn.inspection.DecisionBoundaryDisplay.html\n",
            "   └── sklearn.impute.SimpleImputer.html\n",
            "   └── sklearn.decomposition.dict_learning_online.html\n",
            "   └── sklearn.multioutput.ClassifierChain.html\n",
            "   └── sklearn.utils.validation.check_symmetric.html\n",
            "   └── sklearn.metrics.fbeta_score.html\n",
            "   └── sklearn.feature_selection.mutual_info_regression.html\n",
            "   └── sklearn.datasets.load_breast_cancer.html\n",
            "   └── sklearn.utils.murmurhash3_32.html\n",
            "   └── sklearn.preprocessing.LabelEncoder.html\n",
            "   └── sklearn.feature_extraction.image.extract_patches_2d.html\n",
            "   └── sklearn.utils.random.sample_without_replacement.html\n",
            "   └── sklearn.utils.gen_batches.html\n",
            "   └── sklearn.manifold.TSNE.html\n",
            "   └── sklearn.preprocessing.PowerTransformer.html\n",
            "   └── sklearn.calibration.calibration_curve.html\n",
            "   └── sklearn.metrics.get_scorer.html\n",
            "   └── sklearn.gaussian_process.kernels.Hyperparameter.html\n",
            "   └── sklearn.config_context.html\n",
            "   └── sklearn.utils.metadata_routing.MethodMapping.html\n",
            "   └── sklearn.metrics.mean_absolute_percentage_error.html\n",
            "   └── sklearn.ensemble.StackingClassifier.html\n",
            "   └── sklearn.model_selection.GroupShuffleSplit.html\n",
            "   └── sklearn.cluster.OPTICS.html\n",
            "   └── sklearn.utils.check_scalar.html\n",
            "   └── sklearn.metrics.pairwise.paired_euclidean_distances.html\n",
            "   └── sklearn.preprocessing.scale.html\n",
            "   └── sklearn.feature_selection.SelectPercentile.html\n",
            "   └── sklearn.datasets.load_digits.html\n",
            "   └── sklearn.utils.validation.column_or_1d.html\n",
            "   └── sklearn.preprocessing.power_transform.html\n",
            "   └── sklearn.pipeline.Pipeline.html\n",
            "   └── sklearn.datasets.load_sample_image.html\n",
            "   └── sklearn.neighbors.BallTree.html\n",
            "   └── sklearn.utils.extmath.safe_sparse_dot.html\n",
            "   └── sklearn.base.ClassNamePrefixFeaturesOutMixin.html\n",
            "   └── sklearn.metrics.median_absolute_error.html\n",
            "   └── sklearn.svm.LinearSVR.html\n",
            "   └── sklearn.model_selection.check_cv.html\n",
            "   └── sklearn.metrics.mean_tweedie_deviance.html\n",
            "   └── sklearn.gaussian_process.kernels.Kernel.html\n",
            "   └── sklearn.base.RegressorMixin.html\n",
            "   └── sklearn.tree.export_graphviz.html\n",
            "   └── sklearn.kernel_approximation.Nystroem.html\n",
            "   └── sklearn.metrics.PrecisionRecallDisplay.html\n",
            "   └── sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
            "   └── sklearn.metrics.cluster.contingency_matrix.html\n",
            "   └── sklearn.covariance.EmpiricalCovariance.html\n",
            "   └── sklearn.base.clone.html\n",
            "   └── sklearn.model_selection.ValidationCurveDisplay.html\n",
            "   └── sklearn.gaussian_process.kernels.Product.html\n",
            "   └── sklearn.datasets.fetch_covtype.html\n",
            "   └── sklearn.metrics.coverage_error.html\n",
            "   └── sklearn.model_selection.cross_val_predict.html\n",
            "   └── sklearn.pipeline.make_union.html\n",
            "   └── sklearn.utils.estimator_checks.check_estimator.html\n",
            "   └── sklearn.model_selection.TimeSeriesSplit.html\n",
            "   └── sklearn.metrics.pairwise.nan_euclidean_distances.html\n",
            "   └── sklearn.utils.sparsefuncs.inplace_column_scale.html\n",
            "   └── sklearn.gaussian_process.kernels.ExpSineSquared.html\n",
            "   └── sklearn.datasets.make_multilabel_classification.html\n",
            "   └── sklearn.utils.RegressorTags.html\n",
            "   └── sklearn.utils.multiclass.type_of_target.html\n",
            "   └── sklearn.model_selection.FixedThresholdClassifier.html\n",
            "   └── sklearn.decomposition.SparseCoder.html\n",
            "   └── sklearn.metrics.adjusted_mutual_info_score.html\n",
            "   └── sklearn.metrics.root_mean_squared_error.html\n",
            "   └── sklearn.preprocessing.MinMaxScaler.html\n",
            "   └── sklearn.metrics.pairwise_distances.html\n",
            "   └── sklearn.utils.discovery.all_functions.html\n",
            "   └── sklearn.metrics.zero_one_loss.html\n",
            "   └── sklearn.feature_selection.mutual_info_classif.html\n",
            "   └── sklearn.model_selection.permutation_test_score.html\n",
            "   └── sklearn.metrics.pairwise.additive_chi2_kernel.html\n",
            "   └── sklearn.utils.validation.has_fit_parameter.html\n",
            "   └── sklearn.metrics.hamming_loss.html\n",
            "   └── sklearn.metrics.silhouette_score.html\n",
            "   └── sklearn.linear_model.LassoLarsCV.html\n",
            "   └── sklearn.datasets.load_svmlight_files.html\n",
            "   └── sklearn.model_selection.PredefinedSplit.html\n",
            "   └── sklearn.kernel_approximation.SkewedChi2Sampler.html\n",
            "   └── sklearn.preprocessing.binarize.html\n",
            "   └── sklearn.svm.l1_min_c.html\n",
            "   └── sklearn.decomposition.sparse_encode.html\n",
            "   └── sklearn.metrics.det_curve.html\n",
            "   └── sklearn.metrics.average_precision_score.html\n",
            "   └── sklearn.gaussian_process.GaussianProcessClassifier.html\n",
            "   └── sklearn.metrics.mean_gamma_deviance.html\n",
            "   └── sklearn.cluster.kmeans_plusplus.html\n",
            "   └── sklearn.feature_extraction.image.reconstruct_from_patches_2d.html\n",
            "   └── sklearn.decomposition.FastICA.html\n",
            "   └── sklearn.metrics.PredictionErrorDisplay.html\n",
            "   └── sklearn.metrics.pairwise_distances_argmin_min.html\n",
            "   └── sklearn.isotonic.check_increasing.html\n",
            "   └── sklearn.utils.discovery.all_displays.html\n",
            "   └── sklearn.datasets.make_blobs.html\n",
            "   └── sklearn.covariance.GraphicalLasso.html\n",
            "   └── sklearn.exceptions.DataConversionWarning.html\n",
            "   └── sklearn.utils.as_float_array.html\n",
            "   └── sklearn.utils.InputTags.html\n",
            "   └── sklearn.cluster.MiniBatchKMeans.html\n",
            "   └── sklearn.cluster.SpectralClustering.html\n",
            "   └── sklearn.linear_model.OrthogonalMatchingPursuitCV.html\n",
            "   └── sklearn.exceptions.FitFailedWarning.html\n",
            "   └── sklearn.isotonic.isotonic_regression.html\n",
            "   └── sklearn.covariance.shrunk_covariance.html\n",
            "   └── sklearn.gaussian_process.kernels.Exponentiation.html\n",
            "   └── sklearn.metrics.dcg_score.html\n",
            "   └── sklearn.model_selection.RepeatedKFold.html\n",
            "   └── sklearn.utils.metaestimators.available_if.html\n",
            "   └── sklearn.tree.ExtraTreeRegressor.html\n",
            "   └── sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2.html\n",
            "   └── sklearn.metrics.mean_poisson_deviance.html\n",
            "   └── sklearn.datasets.fetch_species_distributions.html\n",
            "   └── sklearn.decomposition.FactorAnalysis.html\n",
            "   └── sklearn.ensemble.IsolationForest.html\n",
            "   └── sklearn.metrics.label_ranking_average_precision_score.html\n",
            "   └── sklearn.utils.validation.check_memory.html\n",
            "   └── sklearn.decomposition.dict_learning.html\n",
            "   └── sklearn.neighbors.KDTree.html\n",
            "   └── sklearn.cluster.mean_shift.html\n",
            "   └── sklearn.metrics.pairwise.kernel_metrics.html\n",
            "   └── sklearn.metrics.pairwise.cosine_similarity.html\n",
            "   └── sklearn.gaussian_process.kernels.PairwiseKernel.html\n",
            "   └── sklearn.linear_model.ElasticNetCV.html\n",
            "   └── sklearn.metrics.label_ranking_loss.html\n",
            "   └── sklearn.metrics.pairwise.euclidean_distances.html\n",
            "   └── sklearn.cluster.k_means.html\n",
            "   └── sklearn.metrics.pairwise.pairwise_kernels.html\n",
            "   └── sklearn.cluster.AffinityPropagation.html\n",
            "   └── sklearn.neighbors.KNeighborsTransformer.html\n",
            "   └── sklearn.frozen.FrozenEstimator.html\n",
            "   └── sklearn.metrics.auc.html\n",
            "   └── sklearn.utils.Tags.html\n",
            "   └── sklearn.gaussian_process.kernels.Sum.html\n",
            "   └── sklearn.base.DensityMixin.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Slice the documents into smaller chunks\n",
        "\n",
        "In the previous step, we turned each HTML file into a Document. These files may be very long and potentially too large to embed fully. It's also a good practice to avoid embedding large documents:\n",
        "- Long documents often contain several concepts. Retrieval will be easier if each concept is indexed separately.\n",
        "- Retrieved documents will be injected into a prompt, so keeping them short will keep the prompt small(ish).\n",
        "\n",
        "LangChain has a collection of tools to do this: [Text Splitters](https://python.langchain.com/docs/concepts/text_splitters/). In our case, we'll be using the most straightforward one and simplest to use: the [Recursive Character Text Splitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/). The recursive text splitter will recursively reduce the input by splitting it by paragraph, then sentences, then words as needed until the chunk is small enough.\n",
        "\n",
        "### Instructions\n",
        "1. Import the `RecursiveCharacterTextSplitter` from `langchain.text_splitter`.\n",
        "2. Create a text splitter configured with `chunk_size=5000` and `chunk_overlap=200`.  \n",
        "   _These values are arbitrary, and you'll need to try different ones to see which best serve your use case._\n",
        "3. Split the `raw_documents` and store them as `documents`, using the `.split_documents()` method."
      ],
      "metadata": {
        "id": "GUnomOd9shGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader, UnstructuredHTMLLoader\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    path='/content/drive/My Drive/Docs/scikit-learn-docs',\n",
        "    glob='**/*.html',\n",
        "    loader_cls=UnstructuredHTMLLoader\n",
        ")\n",
        "\n",
        "raw_documents = loader.load()\n",
        "print(f\"Total loaded documents: {len(raw_documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LbKNdbudpRFI",
        "outputId": "5ce35d1d-e568-49b6-975b-a182d0968539"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:unstructured:short text: \"sklearn.kernel_ridge#\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"Kernel ridge regression.\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"KernelRidge Kernel ridge regression.\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"previous\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"SkewedChi2Sampler\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"next\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"KernelRidge\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"paired_cosine_distances#\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"previous\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"nan_euclidean_distances\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"next\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"paired_distances\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"On this page\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"This Page\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"Show Source\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"density#\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"Gallery examples#\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"previous\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"unique_labels\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"next\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"fast_logdet\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"On this page\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"This Page\". Defaulting to English.\n",
            "WARNING:unstructured:short text: \"Show Source\". Defaulting to English.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loaded documents: 663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total documents: {len(raw_documents)}\")\n",
        "print(raw_documents[:1])  # Show the first iten (if any)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OQq5J8Vm_by",
        "outputId": "6b9a8793-2685-4677-b07c-621ad6d82572"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents: 663\n",
            "[Document(metadata={'source': '/content/drive/My Drive/Docs/scikit-learn-docs/api/sklearn.kernel_approximation.html'}, page_content='sklearn.kernel_approximation#\\n\\nApproximate kernel feature maps based on Fourier transforms and count sketches.\\n\\nUser guide. See the Kernel Approximation section for further details.\\n\\nAdditiveChi2Sampler Approximate feature map for additive chi2 kernel. Nystroem Approximate a kernel map using a subset of the training data. PolynomialCountSketch Polynomial kernel approximation via Tensor Sketch. RBFSampler Approximate a RBF kernel feature map using random Fourier features. SkewedChi2Sampler Approximate feature map for \"skewed chi-squared\" kernel.\\n\\nprevious\\n\\nisotonic_regression\\n\\nnext\\n\\nAdditiveChi2Sampler')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create the text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Split the documents\n",
        "documents = splitter.split_documents(raw_documents)"
      ],
      "metadata": {
        "id": "1dNMCTZgNCna"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Count Tokens and Estimate Embedding Cost\n",
        "\n",
        "We're now ready to embed our documents. Before we proceed, it's important to understand the size of our data and estimate the cost of embedding. We'll use the [`tiktoken`](https://github.com/openai/tiktoken) library for this purpose. `tiktoken` allows us to encode and decode text into tokens, which is crucial for understanding the token count of our documents.\n",
        "\n",
        "> 💡 To better understand what a token is in the context of GPT, visit [OpenAI's Tokenizer page](https://platform.openai.com/tokenizer) to see how text translates into tokens.\n",
        "\n",
        "You can find the pricing for different models on OpenAI's [pricing page](https://platform.openai.com/docs/pricing).\n",
        "\n",
        "### Instructions\n",
        "1. Import the `tiktoken` library.\n",
        "2. Create a tokenizer for the `text-embedding-3-large` model using the `.encoding_for_model()` method.\n",
        "3. Count the tokens in each document using the `.encode()` method.\n",
        "4. Calculate the total number of tokens.\n",
        "5. Estimate the cost. The `text-embedding-3-large` model costs `$0.13` per 1M tokens."
      ],
      "metadata": {
        "id": "5IjDW_t5tTnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tiktoken\n",
        "import tiktoken\n",
        "\n",
        "# Create an encoder\n",
        "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\") #we can use any snapshot, just browse at: https://platform.openai.com/docs/pricing\n",
        "\n",
        "# Count tokens in each document\n",
        "tokens_per_document = [len(tokenizer.encode(doc.page_content)) for doc in documents]\n",
        "\n",
        "# Calculate the sum of all token counts\n",
        "sum_of_tokens = sum(tokens_per_document)\n",
        "\n",
        "# Calculate a cost estimate\n",
        "estimated_cost = sum_of_tokens/1_000_000 * 0.02\n",
        "print(f\"Estimated cost is {estimated_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXqBLEQ7NpFl",
        "outputId": "3f7f5c9c-704a-4c12-ae55-4d4bc9dae992"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated cost is 0.005041759999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Embed the Documents and Store Embeddings in the Vector Database\n",
        "\n",
        "We are now ready to embed our documents. Since embedding incurs a cost, we will save the embeddings into a database. LangChain simplifies this process using a [Vector Store](https://python.langchain.com/docs/concepts/vectorstores/).\n",
        "\n",
        "There are many vector stores to choose from (see the [full list](https://python.langchain.com/docs/integrations/vectorstores/)). Today, we will use [DuckDB](https://duckdb.org/), but you can use any other as they share the same interface in LangChain. Each vector store has unique features (like metadata filtering), so explore them to find the best fit for your use case.\n",
        "\n",
        "DuckDB is a local analytical database management system designed for fast execution of complex queries. It is particularly well-suited for analytical workloads and can be embedded directly into your application. In addition to its traditional database capabilities, DuckDB supports vector operations, such as similarity search, making it a versatile choice for storing and querying document embeddings. Furthermore, it comes pre-installed in this online notebook environment.\n",
        "\n",
        "### Instructions\n",
        "1. Import `duckdb` and create a database connection using `duckdb.connect`. Store the database in a single file (e.g., `\"embeddings.db\"`), which you can pass to the `connect` function.\n",
        "2. Import `DuckDB` from `langchain_community.vectorstores`.\n",
        "3. Import `OpenAIEmbeddings` from `langchain_openai`.\n",
        "4. Create the embedding function. Set the model to `\"text-embedding-3-large\"` and set a chunk size of `500`. Setting the chunk size is necessary because we have too many documents to embed at once.\n",
        "5. Create a database from our documents using `DuckDB.from_documents()`. Pass the documents, embedding function, the previously created DuckDB connection, and set the table name to `\"embeddings\"`.  \n",
        "   **Warning: Executing this will embed thousands of documents and will cost about $0.005042**"
      ],
      "metadata": {
        "id": "3XblwD89ttZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Substitua pelo valor da sua chave\n",
        "#os.environ[\"TokenAgentKey\"] = \"TokenAgentKey\""
      ],
      "metadata": {
        "id": "WX6RDlZm05FK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DuckDB connection\n",
        "import duckdb\n",
        "conn = duckdb.connect(\"embeddings.db\")\n",
        "\n",
        "# Import the DuckDB vectorstore\n",
        "from langchain_community.vectorstores import DuckDB\n",
        "\n",
        "# Import OpenAIEmbeddings\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Create the embedding function\n",
        "embedding_function = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    chunk_size=500,\n",
        "    openai_api_key=\"Sua chave aqui\"\n",
        ")"
      ],
      "metadata": {
        "id": "8DeB2EK2R4it"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02202799"
      },
      "source": [
        "To use the OpenAI API, you'll need an API key. If you don't already have one, you can create one on the OpenAI website.\n",
        "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel. Give it the name `OPENAI_API_KEY`. Then pass the key to the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a database from the documents and embedding function\n",
        "db = DuckDB.from_documents(\n",
        "    documents=raw_documents,#documents\n",
        "    embedding=embedding_function,\n",
        "    connection=conn,\n",
        "    table_name=\"embeddings\",\n",
        ")"
      ],
      "metadata": {
        "id": "JUIO1MGCl_dQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Query the Vector Database\n",
        "\n",
        "Now that we have a vector database, we can query it. A vector database stores embeddings (vectors) and allows searching through them using the K-Nearest Neighbors algorithm (or a variation of it). When we query it, the following steps will occur:\n",
        "1. Embed the text query to obtain a vector. It is crucial that this embedding is made using the same embedding technique that was used to embed the documents.\n",
        "2. Calculate the distance (or similarity) between the query vector and all other vectors.\n",
        "3. Sort results by similarity.\n",
        "4. Return the most similar documents.\n",
        "\n",
        "To do this with LangChain, we can use the `.similarity_search()` method of the database.\n",
        "\n",
        "### Instructions\n",
        "1. Call the `similarity_search` method on `db` with the search query as a parameter. Store the results in `results`.\n",
        "2. Display the results."
      ],
      "metadata": {
        "id": "Zu4Jp0Du2DUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the `similarity_search_with_score` method on `db`\n",
        "results = db.similarity_search(\"oi\")\n",
        "\n",
        "# Show the results\n",
        "results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7yaDofS2DDR",
        "outputId": "db519ac0-fde1-4992-db72-4b7aac320c4f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/drive/My Drive/Docs/scikit-learn-docs/modules/generated/oas-function.html', '_similarity_score': 0.26869213082374366}, page_content='oas#\\n\\nprevious\\n\\nledoit_wolf_shrinkage\\n\\nnext\\n\\nshrunk_covariance\\n\\nOn this page\\n\\nThis Page\\n\\nShow Source')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Create a LangGraph Graph\n",
        "\n",
        "In this step, we will create a LangGraph graph to handle the retrieval and generation of answers based on our vector database. LangGraph allows us to define a sequence of operations (or states) that our data will go through. We will define two main functions: `retrieve` and `generate`. The `retrieve` function will query our vector database to get the most relevant documents, and the `generate` function will use a language model to generate an answer based on these documents.\n",
        "\n",
        "### Instructions\n",
        "1. We saved you some time and already included the necessary imports.\n",
        "\n",
        "2. Create the prompt template and chat model:\n",
        "    - Use `hub.pull` to pull the prompt template named `\"rlm/rag-prompt\"`. This will download the publicly defined prompt template: https://smith.langchain.com/hub/rlm/rag-prompt.\n",
        "    - Initialize the chat model using `init_chat_model` with the model name `\"gpt-4o-mini\"` and specify the model provider as `\"openai\"`. Set the `temperature` to `0`.\n",
        "\n",
        "3. Set up your State structure:\n",
        "    - Define a class `State` that inherits from `TypedDict`.\n",
        "    - The `State` class should have three fields: `context` (a list of `Document` objects), `question` (a string), and `answer` (a string).\n",
        "\n",
        "4. Define the `retrieve` function which will be used as a `Node` in your graph:\n",
        "    - Create a function named `retrieve` that takes a `state` parameter of type `State`.\n",
        "    - Inside the function, query the vector database using the `similarity_search` method with the question from the state.\n",
        "    - Return a dictionary with the key `\"context\"` and the retrieved documents as the value.\n",
        "\n",
        "5. Define the `generate` function which will be used as a `Node` in your graph:\n",
        "    - Create a function named `generate` that takes a `state` parameter of type `State`.\n",
        "    - Inside the function, concatenate the content of the documents in a single string.\n",
        "    - Use the prompt template to create messages by invoking it with a dictionary containing the concatenated context and the question.\n",
        "    - Generate a response by invoking the chat model with the messages.\n",
        "    - Return a dictionary with the key `\"answer\"` and the generated response as the value.\n",
        "\n",
        "6. Build and compile the graph using the state and the functions:\n",
        "    - Initialize a `StateGraph` object with the `State` class.\n",
        "    - Add a sequence of the `retrieve` and `generate` functions to the graph builder.\n",
        "    - Add an edge from `START` to the `\"retrieve\"` state.\n",
        "    - Compile the graph using the `compile` method of the graph builder.\n",
        "\n",
        "By following these steps, you will create a LangGraph graph that can retrieve relevant documents from the vector database and generate answers using a language model."
      ],
      "metadata": {
        "id": "tYALtE3C2ngM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the necessary imports\n",
        "from langchain import hub\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "# Creat the prompt template and chat model\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "llm = init_chat_model(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    openai_api_key=\"Sua chave aqui\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Set up your State structure\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "# Define the retrieve function\n",
        "def retrieve(state: State):\n",
        "    retrieved_documents = db.similarity_search(\n",
        "        state[\"question\"]\n",
        "    )\n",
        "    return {\n",
        "        \"context\": retrieved_documents,\n",
        "    }\n",
        "\n",
        "# Define the generate function\n",
        "def generate(state: State):\n",
        "    context = \"\\n\\n\".join([\n",
        "        doc.page_content\n",
        "        for doc in state[\"context\"]\n",
        "    ])\n",
        "    messages = prompt.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"context\": context,\n",
        "    })\n",
        "    response = llm.invoke(messages)\n",
        "    return {\n",
        "        \"answer\": response.content,\n",
        "    }\n",
        "\n",
        "# Build and compile the graph using the state and the functions\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, 'retrieve')\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OspPGhiX2pmA",
        "outputId": "b4aa543e-9c59-44ff-9934-b9e4d1ef7743"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the graph with your question\n",
        "response = graph.invoke({\n",
        "    \"question\": \"How can I do k-nearest with Scikit Learn? Answer like a Pirate.\",\n",
        "})\n",
        "\n",
        "# Display the answer\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(response[\"answer\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "YyApocKW3xlx",
        "outputId": "7d9505f7-f65e-4c8e-da23-6ab196426594"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Arrr, to do k-nearest in Scikit Learn, ye use the `KNeighborsClassifier` or `KNeighborsRegressor` for classification and regression, respectively. First, ye fit the model with `fit(X, y)` where X be yer data and y be the labels, then call `kneighbors` to find the closest mates. Ye can also use `kneighbors_graph` to see the neighbor connections, aye!"
          },
          "metadata": {}
        }
      ]
    }
  ]
}